{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNB53ihggFyZ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetchAndSaveToFile(url, path):\n",
        "    r = requests.get(url)\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        f.write(r.text)\n",
        "\n",
        "\n",
        "def scrapeReviewsFromHtml(file_path, csv_path,max_reviews=100):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        soup = BeautifulSoup(f, 'html.parser')\n",
        "\n",
        "    reviews = []\n",
        "    for review in soup.find_all('div', {'data-hook': 'review'}):\n",
        "        username = review.find('span', {'class': 'a-profile-name'}).text.strip()\n",
        "        review_date = review.find('span', {'data-hook': 'review-date'}).text.strip()\n",
        "        review_text = review.find('span', {'data-hook': 'review-body'}).text.strip()\n",
        "        reviews.append([username, review_date, review_text])\n",
        "\n",
        "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Username', 'Review Date', 'Review'])\n",
        "        writer.writerows(reviews)\n",
        "\n",
        "url = \"https://www.amazon.in/Intel-i7-13700K-Desktop-Processor-P-cores/product-reviews/B0BCF57FL5/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"\n",
        "html_path = \"data/times.html\"\n",
        "csv_path = \"data/reviews.csv\"\n",
        "\n",
        "# Fetch and save HTML\n",
        "fetchAndSaveToFile(url, html_path)\n",
        "\n",
        "# Scrape reviews and save to CSV\n",
        "scrapeReviewsFromHtml(html_path, csv_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import time\n",
        "\n",
        "def fetchReviewsFromUrl(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()  # Ensure we notice bad responses\n",
        "    return response.text\n",
        "\n",
        "def parseReviews(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    reviews = []\n",
        "    for review in soup.find_all('div', {'data-hook': 'review'}):\n",
        "        username = review.find('span', {'class': 'a-profile-name'}).text.strip()\n",
        "        review_date = review.find('span', {'data-hook': 'review-date'}).text.strip()\n",
        "        review_text = review.find('span', {'data-hook': 'review-body'}).text.strip()\n",
        "        reviews.append([username, review_date, review_text])\n",
        "    return reviews\n",
        "\n",
        "def scrapeReviewsToCsv(url, csv_path):\n",
        "    base_url = url.split('/ref=')[0]  # Base URL for navigating pages\n",
        "    current_page = 1\n",
        "    all_reviews = []\n",
        "\n",
        "    while True:\n",
        "        print(f\"Scraping page {current_page}...\")\n",
        "        page_url = f\"{base_url}/ref=cm_cr_arp_d_paging_btm_next_{current_page}?ie=UTF8&reviewerType=all_reviews&pageNumber={current_page}\"\n",
        "        html = fetchReviewsFromUrl(page_url)\n",
        "        reviews = parseReviews(html)\n",
        "        if not reviews:\n",
        "            break  # No more reviews to scrape\n",
        "        all_reviews.extend(reviews)\n",
        "        current_page += 1\n",
        "        time.sleep(2)  # Be polite and avoid hammering the server\n",
        "\n",
        "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Username', 'Review Date', 'Review'])\n",
        "        writer.writerows(all_reviews)\n",
        "    print(f\"Finished scraping. Total reviews fetched: {len(all_reviews)}\")\n",
        "\n",
        "url = \"https://www.amazon.in/Intel-i7-13700K-Desktop-Processor-P-cores/product-reviews/B0BCF57FL5/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"\n",
        "csv_path = \"data/reviews.csv\"\n",
        "\n",
        "os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "scrapeReviewsToCsv(url, csv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UmrTdHXxaqK",
        "outputId": "09088d3f-ff00-49b9-9fac-02aabf677138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Finished scraping. Total reviews fetched: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "def fetchReviewsFromUrl(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()  # Ensure we notice bad responses\n",
        "    return response.text\n",
        "\n",
        "def parseReviews(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    reviews = []\n",
        "    for review in soup.find_all('div', {'data-hook': 'review'}):\n",
        "        username = review.find('span', {'class': 'a-profile-name'}).text.strip()\n",
        "        review_date = review.find('span', {'data-hook': 'review-date'}).text.strip()\n",
        "        review_text = review.find('span', {'data-hook': 'review-body'}).text.strip()\n",
        "        reviews.append([username, review_date, review_text])\n",
        "    return reviews\n",
        "\n",
        "def scrapeReviewsToCsv(url, csv_path, max_reviews=1000):\n",
        "    base_url = url.split('/ref=')[0]  # Base URL for navigating pages\n",
        "    current_page = 1\n",
        "    all_reviews = []\n",
        "\n",
        "    while len(all_reviews) < max_reviews:\n",
        "        print(f\"Scraping page {current_page}...\")\n",
        "        page_url = f\"{base_url}/ref=cm_cr_getr_d_paging_btm_next_{current_page}?ie=UTF8&reviewerType=all_reviews&pageNumber={current_page}\"\n",
        "        html = fetchReviewsFromUrl(page_url)\n",
        "        reviews = parseReviews(html)\n",
        "        if not reviews:\n",
        "            break  # No more reviews to scrape\n",
        "        all_reviews.extend(reviews)\n",
        "        current_page += 1\n",
        "        time.sleep(2)  # Be polite and avoid hammering the server\n",
        "\n",
        "    # Trim to the max_reviews count\n",
        "    all_reviews = all_reviews[:max_reviews]\n",
        "\n",
        "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Username', 'Review Date', 'Review'])\n",
        "        writer.writerows(all_reviews)\n",
        "    print(f\"Finished scraping. Total reviews fetched: {len(all_reviews)}\")\n",
        "\n",
        "url = \"https://www.amazon.in/Intel-i7-13700K-Desktop-Processor-P-cores/product-reviews/B0BCF57FL5/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"\n",
        "csv_path = \"data/reviews.csv\"\n",
        "\n",
        "os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "scrapeReviewsToCsv(url, csv_path, max_reviews=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDKW0ankyoKw",
        "outputId": "70496937-4c7a-4f54-e50d-8262cec0e495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Finished scraping. Total reviews fetched: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "def fetchReviewsFromUrl(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()  # Ensure we notice bad responses\n",
        "    return response.text\n",
        "\n",
        "def parseReviews(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    reviews = []\n",
        "    for review in soup.find_all('div', {'data-hook': 'review'}):\n",
        "        username = review.find('span', {'class': 'a-profile-name'}).text.strip()\n",
        "        review_date = review.find('span', {'data-hook': 'review-date'}).text.strip()\n",
        "        review_text = review.find('span', {'data-hook': 'review-body'}).text.strip()\n",
        "        reviews.append([username, review_date, review_text])\n",
        "    return reviews\n",
        "\n",
        "def scrapeReviewsToCsv(url, csv_path, max_reviews=1000):\n",
        "    base_url = url.split('/ref=')[0]  # Base URL for navigating pages\n",
        "    current_page = 1\n",
        "    all_reviews = []\n",
        "\n",
        "    while len(all_reviews) < max_reviews:\n",
        "        print(f\"Scraping page {current_page}...\")\n",
        "        page_url = f\"{base_url}/ref=cm_cr_getr_d_paging_btm_next_{current_page}?ie=UTF8&reviewerType=all_reviews&pageNumber={current_page}\"\n",
        "        html = fetchReviewsFromUrl(page_url)\n",
        "        reviews = parseReviews(html)\n",
        "        if not reviews:\n",
        "            break  # No more reviews to scrape\n",
        "        all_reviews.extend(reviews)\n",
        "        current_page += 1\n",
        "        time.sleep(2)  # Be polite and avoid hammering the server\n",
        "\n",
        "    # Trim to the max_reviews count\n",
        "    all_reviews = all_reviews[:max_reviews]\n",
        "\n",
        "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Username', 'Review Date', 'Review'])\n",
        "        writer.writerows(all_reviews)\n",
        "    print(f\"Finished scraping. Total reviews fetched: {len(all_reviews)}\")\n",
        "\n",
        "url = \"https://www.amazon.in/Intel-i7-13700K-Desktop-Processor-P-cores/product-reviews/B0BCF57FL5/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"\n",
        "csv_path = \"data/reviews.csv\"\n",
        "\n",
        "os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "scrapeReviewsToCsv(url, csv_path, max_reviews=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s-NQ9vT1M8M",
        "outputId": "ce54a6f4-b034-4b96-844d-16b988adf7f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Finished scraping. Total reviews fetched: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "def fetchReviewsFromUrl(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "\n",
        "def parseReviews(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    reviews = []\n",
        "    for review in soup.find_all('div', {'data-hook': 'review'}):\n",
        "        username = review.find('span', {'class': 'a-profile-name'}).text.strip()\n",
        "        review_date = review.find('span', {'data-hook': 'review-date'}).text.strip()\n",
        "        review_text = review.find('span', {'data-hook': 'review-body'}).text.strip()\n",
        "        reviews.append([username, review_date, review_text])\n",
        "    return reviews\n",
        "\n",
        "def scrapeReviewsToCsv(base_url, csv_path, max_reviews=1000):\n",
        "    current_page = 1\n",
        "    all_reviews = []\n",
        "\n",
        "    while len(all_reviews) < max_reviews:\n",
        "        print(f\"Scraping page {current_page}...\")\n",
        "        page_url = f\"{base_url}&pageNumber={current_page}\"\n",
        "        html = fetchReviewsFromUrl(page_url)\n",
        "        reviews = parseReviews(html)\n",
        "        if not reviews:\n",
        "            break  # No more reviews to scrape\n",
        "        all_reviews.extend(reviews)\n",
        "        current_page += 1\n",
        "        time.sleep(2)  # Be polite and avoid hammering the server\n",
        "\n",
        "    # Trim to the max_reviews count\n",
        "    all_reviews = all_reviews[:max_reviews]\n",
        "\n",
        "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Username', 'Review Date', 'Review'])\n",
        "        writer.writerows(all_reviews)\n",
        "    print(f\"Finished scraping. Total reviews fetched: {len(all_reviews)}\")\n",
        "\n",
        "base_url = \"https://www.amazon.in/Intel-i7-13700K-Desktop-Processor-P-cores/product-reviews/B0BCF57FL5/ref=cm_cr_arp_d_paging_btm_prev_2?ie=UTF8&reviewerType=all_reviews&pageNumber=2\"\n",
        "csv_path = \"data/reviews.csv\"\n",
        "\n",
        "os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "scrapeReviewsToCsv(base_url, csv_path, max_reviews=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9U8wmuh2-M8W",
        "outputId": "d7b727d8-ae9c-4d03-bd8d-cd3173b00c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Finished scraping. Total reviews fetched: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "def fetchReviewsFromUrl(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()  # Ensure we notice bad responses\n",
        "    return response.text\n",
        "\n",
        "def parseReviews(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    reviews = []\n",
        "    for review in soup.find_all('div', {'data-hook': 'review'}):\n",
        "        username = review.find('span', {'class': 'a-profile-name'}).text.strip()\n",
        "        review_date = review.find('span', {'data-hook': 'review-date'}).text.strip()\n",
        "        review_text = review.find('span', {'data-hook': 'review-body'}).text.strip()\n",
        "        reviews.append([username, review_date, review_text])\n",
        "    return reviews\n",
        "\n",
        "def scrapeReviewsToCsv(base_url, csv_path, max_reviews=1000):\n",
        "    current_page = 1\n",
        "    all_reviews = []\n",
        "\n",
        "    while len(all_reviews) < max_reviews:\n",
        "        print(f\"Scraping page {current_page}...\")\n",
        "        page_url = f\"{base_url}&pageNumber={current_page}\"\n",
        "        html = fetchReviewsFromUrl(page_url)\n",
        "        reviews = parseReviews(html)\n",
        "        if not reviews:\n",
        "            break  # No more reviews to scrape\n",
        "        all_reviews.extend(reviews)\n",
        "        current_page += 1\n",
        "        time.sleep(2)  # Be polite and avoid hammering the server\n",
        "\n",
        "    # Trim to the max_reviews count\n",
        "    all_reviews = all_reviews[:max_reviews]\n",
        "\n",
        "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Username', 'Review Date', 'Review'])\n",
        "        writer.writerows(all_reviews)\n",
        "    print(f\"Finished scraping. Total reviews fetched: {len(all_reviews)}\")\n",
        "\n",
        "# Adjusted base URL for pagination\n",
        "base_url = \"https://www.amazon.in/Intel-i7-13700K-Desktop-Processor-P-cores/product-reviews/B0BCF57FL5/ref=cm_cr_arp_d_paging_btm_prev_2?ie=UTF8&reviewerType=all_reviews\"\n",
        "csv_path = \"data/reviews.csv\"\n",
        "\n",
        "os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "scrapeReviewsToCsv(base_url, csv_path, max_reviews=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wrddoIY-46k",
        "outputId": "0367c6af-a788-4307-a74f-61c67073b730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Finished scraping. Total reviews fetched: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetchAndSaveToFile(url, path):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",
        "    }\n",
        "    r = requests.get(url, headers=headers)\n",
        "    r.raise_for_status()  # Ensure we notice bad responses\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        f.write(r.text)\n",
        "\n",
        "\n",
        "def scrapeReviewsFromHtml(file_path, csv_path, max_reviews=100):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        soup = BeautifulSoup(f, 'html.parser')\n",
        "\n",
        "    reviews = []\n",
        "    for review in soup.find_all('div', {'data-hook': 'review'}):\n",
        "        try:\n",
        "            username = review.find('span', {'class': 'a-profile-name'}).text.strip()\n",
        "            review_date = review.find('span', {'data-hook': 'review-date'}).text.strip()\n",
        "            review_text = review.find('span', {'data-hook': 'review-body'}).text.strip()\n",
        "            reviews.append([username, review_date, review_text])\n",
        "        except AttributeError:\n",
        "            continue  # Skip if any of the expected fields are not found\n",
        "\n",
        "    if reviews:\n",
        "        with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['Username', 'Review Date', 'Review'])\n",
        "            writer.writerows(reviews)\n",
        "        print(f\"Successfully scraped {len(reviews)} reviews.\")\n",
        "    else:\n",
        "        print(\"No reviews found.\")\n",
        "\n",
        "\n",
        "url = \"https://www.amazon.in/Intel-i7-13700K-Desktop-Processor-P-cores/product-reviews/B0BCF57FL5/ref=cm_cr_getr_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=2\"\n",
        "html_path = \"data/times.html\"\n",
        "csv_path = \"data/reviews2.csv\"\n",
        "\n",
        "# Fetch and save HTML\n",
        "fetchAndSaveToFile(url, html_path)\n",
        "\n",
        "# Scrape reviews and save to CSV\n",
        "scrapeReviewsFromHtml(html_path, csv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oXm5Jtx_hoy",
        "outputId": "e6440837-ffbb-4335-8dd9-0a006ce19589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No reviews found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "def fetchReviewsFromUrl(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()  # Ensure we notice bad responses\n",
        "    return response.text\n",
        "\n",
        "def parseReviews(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    reviews = []\n",
        "    for review in soup.find_all('div', {'data-hook': 'review'}):\n",
        "        try:\n",
        "            username = review.find('span', {'class': 'a-profile-name'}).text.strip()\n",
        "            review_date = review.find('span', {'data-hook': 'review-date'}).text.strip()\n",
        "            review_text = review.find('span', {'data-hook': 'review-body'}).text.strip()\n",
        "            reviews.append([username, review_date, review_text])\n",
        "        except AttributeError:\n",
        "            continue  # Skip if any of the expected fields are not found\n",
        "    return reviews\n",
        "\n",
        "def scrapeReviewsToCsv(url, csv_path, max_reviews=1000):\n",
        "    base_url = url.split('/ref=')[0]  # Base URL for navigating pages\n",
        "    all_reviews = []\n",
        "    current_page = 1\n",
        "\n",
        "    while len(all_reviews) < max_reviews:\n",
        "        print(f\"Scraping page {current_page}...\")\n",
        "        page_url = f\"{base_url}/ref=cm_cr_getr_d_paging_btm_next_{current_page}?ie=UTF8&reviewerType=all_reviews&pageNumber={current_page}\"\n",
        "        html = fetchReviewsFromUrl(page_url)\n",
        "        reviews = parseReviews(html)\n",
        "        if not reviews:\n",
        "            break  # No more reviews to scrape\n",
        "        all_reviews.extend(reviews)\n",
        "        current_page += 1\n",
        "        time.sleep(2)  # Be polite and avoid hammering the server\n",
        "\n",
        "    # Trim to the max_reviews count\n",
        "    all_reviews = all_reviews[:max_reviews]\n",
        "\n",
        "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Username', 'Review Date', 'Review'])\n",
        "        writer.writerows(all_reviews)\n",
        "    print(f\"Finished scraping. Total reviews fetched: {len(all_reviews)}\")\n",
        "\n",
        "url = \"https://www.amazon.in/Intel-i7-13700K-Desktop-Processor-P-cores/product-reviews/B0BCF57FL5/ref=cm_cr_getr_d_paging_btm_next_1?ie=UTF8&reviewerType=all_reviews&pageNumber=2\"\n",
        "csv_path = \"data/reviews.csv\"\n",
        "\n",
        "os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "scrapeReviewsToCsv(url, csv_path, max_reviews=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqObDbiOp7L4",
        "outputId": "d1297bc5-aab1-4a33-8ae9-803828efa712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Finished scraping. Total reviews fetched: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetchPageContent(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()  # Ensure we notice bad responses\n",
        "    return response.text\n",
        "\n",
        "def parseReviews(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    reviews = []\n",
        "    for review in soup.find_all('div', {'data-hook': 'review'}):\n",
        "        try:\n",
        "            username = review.find('span', {'class': 'a-profile-name'}).text.strip()\n",
        "            review_date = review.find('span', {'data-hook': 'review-date'}).text.strip()\n",
        "            review_text = review.find('span', {'data-hook': 'review-body'}).text.strip()\n",
        "            reviews.append([username, review_date, review_text])\n",
        "        except AttributeError:\n",
        "            continue  # Skip if any of the expected fields are not found\n",
        "    return reviews\n",
        "\n",
        "def saveReviewsToCsv(reviews, csv_path):\n",
        "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Username', 'Review Date', 'Review'])\n",
        "        writer.writerows(reviews)\n",
        "    print(f\"Successfully saved {len(reviews)} reviews to {csv_path}\")\n",
        "\n",
        "# URL of the product reviews on page 2\n",
        "url = \"https://www.amazon.in/Intel-i7-13700K-Desktop-Processor-P-cores/product-reviews/B0BCF57FL5/ref=cm_cr_getr_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=2\"\n",
        "csv_path = \"data/reviews_page_2.csv\"\n",
        "\n",
        "# Fetch HTML content of page 2\n",
        "html_content = fetchPageContent(url)\n",
        "\n",
        "# Parse reviews from the HTML content\n",
        "reviews = parseReviews(html_content)\n",
        "\n",
        "# Save the reviews to a CSV file\n",
        "saveReviewsToCsv(reviews, csv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOc3BHx7q0KR",
        "outputId": "23950b17-11f3-4bc7-c6ca-f107c23ea9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved 0 reviews to data/reviews_page_2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetchReviewsFromUrl(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()  # Ensure we notice bad responses\n",
        "    return response.text\n",
        "\n",
        "def parseReviews(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    reviews = []\n",
        "    for review in soup.find_all('div', {'data-hook': 'review'}):\n",
        "        try:\n",
        "            username = review.find('span', {'class': 'a-profile-name'}).text.strip()\n",
        "            review_date = review.find('span', {'data-hook': 'review-date'}).text.strip()\n",
        "            review_text = review.find('span', {'data-hook': 'review-body'}).text.strip()\n",
        "            reviews.append([username, review_date, review_text])\n",
        "        except AttributeError:\n",
        "            continue  # Skip if any of the expected fields are not found\n",
        "    return reviews\n",
        "\n",
        "def scrapeReviewsFromPageToCsv(url, csv_path):\n",
        "    html = fetchReviewsFromUrl(url)\n",
        "    reviews = parseReviews(html)\n",
        "\n",
        "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Username', 'Review Date', 'Review'])\n",
        "        writer.writerows(reviews)\n",
        "    print(f\"Finished scraping. Total reviews fetched: {len(reviews)}\")\n",
        "\n",
        "# URL for page 3 of the reviews\n",
        "url = \"https://www.amazon.in/Intel-i7-13700K-Desktop-Processor-P-cores/product-reviews/B0BCF57FL5/ref=cm_cr_getr_d_paging_btm_next_3?ie=UTF8&reviewerType=all_reviews&pageNumber=3\"\n",
        "csv_path = \"data/reviews_page_3.csv\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "\n",
        "# Scrape reviews from page 3 and save to CSV\n",
        "scrapeReviewsFromPageToCsv(url, csv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLiFccCHrGu4",
        "outputId": "6e8b8004-6f9d-4c67-9c32-cca00ab81c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished scraping. Total reviews fetched: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetchAndSaveToFile(url, path):\n",
        "    r = requests.get(url)\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        f.write(r.text)\n",
        "\n",
        "\n",
        "def scrapeReviewsFromHtml(file_path, csv_path,max_reviews=100):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        soup = BeautifulSoup(f, 'html.parser')\n",
        "\n",
        "    reviews = []\n",
        "    for review in soup.find_all('div', {'data-hook': 'review'}):\n",
        "        username = review.find('span', {'class': 'a-profile-name'}).text.strip()\n",
        "        review_date = review.find('span', {'data-hook': 'review-date'}).text.strip()\n",
        "        review_text = review.find('span', {'data-hook': 'review-body'}).text.strip()\n",
        "        reviews.append([username, review_date, review_text])\n",
        "\n",
        "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Username', 'Review Date', 'Review'])\n",
        "        writer.writerows(reviews)\n",
        "\n",
        "url = \"https://www.amazon.in/Intel-Generation-Desktop-Processor-Warranty/product-reviews/B09MDFH5HY/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"\n",
        "html_path = \"data/times.html\"\n",
        "csv_path = \"data/reviewsi5.csv\"\n",
        "\n",
        "# Fetch and save HTML\n",
        "fetchAndSaveToFile(url, html_path)\n",
        "\n",
        "# Scrape reviews and save to CSV\n",
        "scrapeReviewsFromHtml(html_path, csv_path)\n"
      ],
      "metadata": {
        "id": "easz8nWtd8Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetchAndSaveToFile(url, path):\n",
        "    r = requests.get(url)\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        f.write(r.text)\n",
        "\n",
        "\n",
        "def scrapeReviewsFromHtml(file_path, csv_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        soup = BeautifulSoup(f, 'html.parser')\n",
        "\n",
        "    reviews = []\n",
        "    for review in soup.find_all('div', {'data-hook': 'review'}):\n",
        "        username = review.find('span', {'class': 'a-profile-name'}).text.strip()\n",
        "        review_date = review.find('span', {'data-hook': 'review-date'}).text.strip()\n",
        "        review_text = review.find('span', {'data-hook': 'review-body'}).text.strip()\n",
        "        reviews.append([username, review_date, review_text])\n",
        "\n",
        "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Username', 'Review Date', 'Review'])\n",
        "        writer.writerows(reviews)\n",
        "\n",
        "url = \"https://www.amazon.in/Intel-Core-i9-12900KS-Hexadeca-core-Processor/product-reviews/B09RWL74GY/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"\n",
        "html_path = \"data/times.html\"\n",
        "csv_path = \"data/reviewsi94.csv\"\n",
        "\n",
        "# Fetch and save HTML\n",
        "fetchAndSaveToFile(url, html_path)\n",
        "\n",
        "# Scrape reviews and save to CSV\n",
        "scrapeReviewsFromHtml(html_path, csv_path)"
      ],
      "metadata": {
        "id": "PIreQ4P67HpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# List of new files to be appended\n",
        "new_files = [\n",
        "    \"reviewsi9.csv\",\n",
        "    \"reviewsi91.csv\",\n",
        "    \"reviewsi92.csv\",\n",
        "    \"reviewsi93.csv\",\n",
        "    \"reviewsi94.csv\"\n",
        "]\n",
        "\n",
        "# Path to the existing combined file\n",
        "combined_file = \"reviewsi_combined.csv\"\n",
        "\n",
        "def appendCsvFiles(combined_file, new_files):\n",
        "    # Open the combined file in append mode\n",
        "    with open(combined_file, 'a', newline='', encoding='utf-8') as outfile:\n",
        "        writer = csv.writer(outfile)\n",
        "\n",
        "        for new_file in new_files:\n",
        "            with open(new_file, 'r', encoding='utf-8') as infile:\n",
        "                reader = csv.reader(infile)\n",
        "\n",
        "                # Skip the header of the new file\n",
        "                next(reader)\n",
        "\n",
        "                # Write the rows from the new file to the combined file\n",
        "                for row in reader:\n",
        "                    writer.writerow(row)\n",
        "\n",
        "# Append the new files to the combined file\n",
        "appendCsvFiles(combined_file, new_files)\n",
        "\n",
        "# Download the updated combined file\n",
        "from google.colab import files\n",
        "files.download(combined_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "joilxuVcA8D8",
        "outputId": "326dbf53-54e6-4a3b-e9a8-26820152d868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b80272b9-4977-4d37-8d3e-3923a62bc38f\", \"reviewsi_combined.csv\", 18388)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}